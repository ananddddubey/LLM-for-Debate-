{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gradio\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Optional, Union\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, Trainer, TrainingArguments\nfrom sentence_transformers import SentenceTransformer\nimport logging\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport datasets\nfrom torch.utils.data import Dataset\nimport json\nimport gradio as gr\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass ArgumentRelationDataset(Dataset):\n    \"\"\"Custom dataset for argument relation data\"\"\"\n    \n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n        \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        source = item[\"source_text\"]\n        target = item[\"target_text\"]\n        context = item.get(\"context\", \"\")\n        \n        # Prepare text input\n        if context:\n            text = f\"Context: {context}\\nSource: {source}\\nTarget: {target}\"\n        else:\n            text = f\"Source: {source}\\nTarget: {target}\"\n            \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Remove batch dimension\n        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n        \n        # Add label\n        encoding[\"labels\"] = torch.tensor(item[\"label_id\"])\n        \n        return encoding\n\nclass EnhancedRBAMModel:\n    \"\"\"\n    Enhanced Relation-Based Argument Mining Model\n    \n    This model combines transformer-based classification with semantic similarity\n    analysis and contextual argument clustering to provide more accurate relation\n    detection between arguments.\n    \"\"\"\n    \n    def __init__(self, \n                 model_path: str = \"models/deberta-v3-large-relation-finetuned\",\n                 embedding_model: str = \"all-MiniLM-L6-v2\",\n                 device: str = None):\n        \"\"\"\n        Initialize the enhanced RBAM model.\n        \n        Args:\n            model_path: Path to the fine-tuned classification model\n            embedding_model: SentenceTransformer model for semantic analysis\n            device: Device to run the model on (None for auto-detection)\n        \"\"\"\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Using device: {self.device}\")\n        \n        # Load classification model\n        try:\n            self.model_path = model_path\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n            self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)\n            self.model.eval()\n            logger.info(f\"Successfully loaded classification model from {model_path}\")\n            \n            # Determine relation labels from model config\n            self.id2label = self.model.config.id2label if hasattr(self.model.config, 'id2label') else {\n                0: \"support\", 1: \"attack\", 2: \"neutral\", 3: \"detail\"\n            }\n            self.label2id = {v: k for k, v in self.id2label.items()}\n            logger.info(f\"Loaded relation labels: {self.id2label}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load classification model: {e}\")\n            raise Exception(f\"Model initialization failed: {str(e)}\")\n            \n        # Load embedding model for semantic analysis\n        try:\n            self.embedding_model = SentenceTransformer(embedding_model).to(self.device)\n            logger.info(f\"Successfully loaded embedding model: {embedding_model}\")\n        except Exception as e:\n            logger.error(f\"Failed to load embedding model: {e}\")\n            self.embedding_model = None\n            \n        # Initialize cache\n        self.cache = {}\n        \n    def train_model(self, train_data: List[Dict], validation_data: Optional[List[Dict]] = None, \n                   output_dir: str = \"./model_output\", epochs: int = 3, batch_size: int = 16):\n        \"\"\"\n        Train or fine-tune the model on new data.\n        \n        Args:\n            train_data: List of dictionaries with source_text, target_text, optional context, and label_id\n            validation_data: Optional validation data with the same format as train_data\n            output_dir: Directory to save the trained model\n            epochs: Number of training epochs\n            batch_size: Batch size for training\n        \"\"\"\n        logger.info(f\"Starting model training with {len(train_data)} examples\")\n        \n        # Create datasets\n        train_dataset = ArgumentRelationDataset(train_data, self.tokenizer)\n        eval_dataset = ArgumentRelationDataset(validation_data, self.tokenizer) if validation_data else None\n        \n        # Define training arguments\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=100,\n            evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True if eval_dataset else False,\n        )\n        \n        # Create trainer\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset\n        )\n        \n        # Train model\n        trainer.train()\n        \n        # Save model\n        self.model.save_pretrained(output_dir)\n        self.tokenizer.save_pretrained(output_dir)\n        \n        # Update current model\n        self.model = trainer.model\n        logger.info(f\"Training completed, model saved to {output_dir}\")\n        \n        # Clear cache after training\n        self.cache = {}\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Model trained on {len(train_data)} examples and saved to {output_dir}\"\n        }\n        \n    def predict_relation(self, \n                        source_text: str, \n                        target_text: str, \n                        context: Optional[str] = None) -> Dict:\n        \"\"\"\n        Predict the relation between source and target arguments.\n        \n        Args:\n            source_text: The source argument text\n            target_text: The target argument text\n            context: Optional context for the arguments\n            \n        Returns:\n            Dictionary with relation type, confidence score, and features\n        \"\"\"\n        # Create cache key\n        cache_key = f\"{source_text[:100]}|{target_text[:100]}|{context[:100] if context else 'no_context'}\"\n        \n        # Check if result is cached\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n            \n        # Prepare input\n        input_text = self._prepare_input(source_text, target_text, context)\n        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n        \n        # Get model prediction\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits\n            probabilities = torch.nn.functional.softmax(logits, dim=1)[0]\n            prediction = torch.argmax(probabilities).item()\n            confidence = probabilities[prediction].item()\n            \n        # Get additional semantic features if embedding model is available\n        semantic_features = {}\n        if self.embedding_model:\n            semantic_features = self._get_semantic_features(source_text, target_text)\n        \n        # Ensemble the predictions\n        relation_type = self.id2label[prediction]\n        \n        # Apply confidence adjustment based on semantic features\n        if semantic_features and 'similarity' in semantic_features:\n            # Adjust confidence based on semantic similarity\n            if relation_type in ['support', 'detail'] and semantic_features['similarity'] < 0.3:\n                confidence = confidence * 0.8  # Reduce confidence for support/detail with low similarity\n            elif relation_type == 'attack' and semantic_features['similarity'] > 0.8:\n                confidence = confidence * 0.8  # Reduce confidence for attack with high similarity\n        \n        # Build result\n        result = {\n            'relation_type': relation_type,\n            'confidence': confidence,\n            'features': {\n                'probabilities': {self.id2label[i]: prob.item() for i, prob in enumerate(probabilities)},\n                **semantic_features\n            }\n        }\n        \n        # Cache the result\n        self.cache[cache_key] = result\n        \n        return result\n    \n    def _prepare_input(self, source_text: str, target_text: str, context: Optional[str] = None) -> str:\n        \"\"\"\n        Prepare input text for the model.\n        \"\"\"\n        if context:\n            return f\"Context: {context}\\nSource: {source_text}\\nTarget: {target_text}\"\n        else:\n            return f\"Source: {source_text}\\nTarget: {target_text}\"\n    \n    def _get_semantic_features(self, source_text: str, target_text: str) -> Dict:\n        \"\"\"\n        Extract semantic features for the argument pair.\n        \"\"\"\n        # Get embeddings\n        source_embedding = self.embedding_model.encode(source_text, convert_to_tensor=True)\n        target_embedding = self.embedding_model.encode(target_text, convert_to_tensor=True)\n        \n        # Calculate similarity\n        similarity = torch.cosine_similarity(source_embedding.unsqueeze(0), \n                                           target_embedding.unsqueeze(0)).item()\n        \n        return {\n            'similarity': similarity,\n        }\n    \n    def batch_predict(self, argument_pairs: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Predict relations for multiple argument pairs.\n        \n        Args:\n            argument_pairs: List of dictionaries with source_text, target_text, and optional context\n            \n        Returns:\n            List of prediction dictionaries\n        \"\"\"\n        results = []\n        \n        for pair in argument_pairs:\n            source = pair.get('source_text')\n            target = pair.get('target_text')\n            context = pair.get('context')\n            \n            if not source or not target:\n                results.append({'error': 'Missing source or target text'})\n                continue\n                \n            result = self.predict_relation(source, target, context)\n            results.append(result)\n            \n        return results\n    \n    def analyze_graph(self, argument_pairs: List[Dict]) -> Dict:\n        \"\"\"\n        Perform graph-based analysis of the argument network.\n        \n        Args:\n            argument_pairs: List of dictionaries with source_text, target_text, and optional context\n            \n        Returns:\n            Dictionary with graph analysis results\n        \"\"\"\n        # Get predictions for all pairs\n        predictions = self.batch_predict(argument_pairs)\n        \n        # Extract unique arguments\n        unique_args = set()\n        for pair in argument_pairs:\n            unique_args.add(pair['source_text'])\n            unique_args.add(pair['target_text'])\n        \n        # Build adjacency matrix\n        n_args = len(unique_args)\n        arg_list = list(unique_args)\n        arg_to_idx = {arg: i for i, arg in enumerate(arg_list)}\n        \n        # Initialize matrices\n        support_matrix = np.zeros((n_args, n_args))\n        attack_matrix = np.zeros((n_args, n_args))\n        \n        # Fill matrices\n        for i, pair in enumerate(argument_pairs):\n            source_idx = arg_to_idx[pair['source_text']]\n            target_idx = arg_to_idx[pair['target_text']]\n            rel_type = predictions[i]['relation_type']\n            confidence = predictions[i]['confidence']\n            \n            if rel_type == 'support':\n                support_matrix[source_idx, target_idx] = confidence\n            elif rel_type == 'attack':\n                attack_matrix[source_idx, target_idx] = confidence\n        \n        # Calculate centrality metrics\n        support_centrality = np.sum(support_matrix, axis=0)\n        attack_centrality = np.sum(attack_matrix, axis=0)\n        \n        # Identify key arguments\n        key_args = []\n        for i, arg in enumerate(arg_list):\n            total_centrality = support_centrality[i] + attack_centrality[i]\n            if total_centrality > 0:\n                key_args.append({\n                    'text': arg[:100] + '...' if len(arg) > 100 else arg,\n                    'support_centrality': float(support_centrality[i]),\n                    'attack_centrality': float(attack_centrality[i]),\n                    'total_centrality': float(total_centrality)\n                })\n        \n        # Sort by total centrality\n        key_args.sort(key=lambda x: x['total_centrality'], reverse=True)\n        \n        # Identify inconsistencies\n        inconsistencies = self._identify_inconsistencies(argument_pairs, predictions)\n        \n        return {\n            'key_arguments': key_args[:5],  # Top 5 key arguments\n            'argument_count': len(unique_args),\n            'relation_count': len(argument_pairs),\n            'inconsistencies': inconsistencies\n        }\n    \n    def _identify_inconsistencies(self, argument_pairs: List[Dict], predictions: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Identify potential inconsistencies in the argument network.\n        \"\"\"\n        inconsistencies = []\n        \n        # Build graph\n        graph = {}\n        for i, pair in enumerate(argument_pairs):\n            source = pair['source_text']\n            target = pair['target_text']\n            rel_type = predictions[i]['relation_type']\n            \n            if source not in graph:\n                graph[source] = {}\n            if target not in graph:\n                graph[target] = {}\n                \n            graph[source][target] = rel_type\n        \n        # Check for circular support/attack patterns\n        for source in graph:\n            for target in graph.get(source, {}):\n                rel_type = graph[source][target]\n                \n                # Check if there's a reverse relation\n                if target in graph and source in graph.get(target, {}):\n                    reverse_rel = graph[target][source]\n                    \n                    # Check if there's an inconsistency\n                    if rel_type == reverse_rel and rel_type in ['support', 'attack']:\n                        inconsistencies.append({\n                            'type': 'circular_relation',\n                            'source': source[:50] + '...' if len(source) > 50 else source,\n                            'target': target[:50] + '...' if len(target) > 50 else target,\n                            'relation': rel_type,\n                            'description': f\"Circular {rel_type} relation detected\"\n                        })\n        \n        # Check for support-attack contradictions\n        for arg1 in graph:\n            for arg2 in graph.get(arg1, {}):\n                rel1 = graph[arg1][arg2]\n                \n                # Check if arg1 and arg2 have relations to the same arg3\n                for arg3 in graph:\n                    if arg3 != arg1 and arg3 != arg2 and arg2 in graph.get(arg3, {}):\n                        rel2 = graph[arg3][arg2]\n                        \n                        # Check if one supports while the other attacks\n                        if rel1 == 'support' and rel2 == 'attack' or rel1 == 'attack' and rel2 == 'support':\n                            inconsistencies.append({\n                                'type': 'competing_relations',\n                                'arguments': [\n                                    arg1[:50] + '...' if len(arg1) > 50 else arg1,\n                                    arg2[:50] + '...' if len(arg2) > 50 else arg2,\n                                    arg3[:50] + '...' if len(arg3) > 50 else arg3\n                                ],\n                                'description': f\"Competing {rel1}/{rel2} relations detected\"\n                            })\n        \n        return inconsistencies\n\n# Create a function to run the interactive web UI\ndef run_interactive_rbam_system():\n    # Create the model\n    model_path = \"distilbert-base-uncased\"  # Default model, will be fine-tuned\n    \n    try:\n        rbam_model = EnhancedRBAMModel(model_path=model_path)\n        model_status = \"Model loaded successfully\"\n    except Exception as e:\n        # If loading fails, use a fallback approach\n        from transformers import DistilBertForSequenceClassification\n        model = DistilBertForSequenceClassification.from_pretrained(\n            \"distilbert-base-uncased\", \n            num_labels=4\n        )\n        model.config.id2label = {0: \"support\", 1: \"attack\", 2: \"neutral\", 3: \"detail\"}\n        model.config.label2id = {\"support\": 0, \"attack\": 1, \"neutral\": 2, \"detail\": 3}\n        \n        # Create output directory if it doesn't exist\n        os.makedirs(\"models/distilbert-rbam\", exist_ok=True)\n        model.save_pretrained(\"models/distilbert-rbam\")\n        \n        # Now initialize with the saved model\n        rbam_model = EnhancedRBAMModel(model_path=\"models/distilbert-rbam\")\n        model_status = \"Initialized with base model (requires training)\"\n    \n    # Sample training data\n    sample_train_data = [\n        {\n            \"source_text\": \"Climate change is primarily caused by human activities.\",\n            \"target_text\": \"The rise in global temperatures correlates with increased CO2 emissions.\",\n            \"context\": \"Environmental science debate\",\n            \"label_id\": 0  # support\n        },\n        {\n            \"source_text\": \"Renewable energy is too expensive to replace fossil fuels.\",\n            \"target_text\": \"Solar panel costs have decreased by 90% in the last decade.\",\n            \"context\": \"Energy policy discussion\",\n            \"label_id\": 1  # attack\n        },\n        {\n            \"source_text\": \"Excessive social media use is harmful to mental health.\",\n            \"target_text\": \"Studies show correlation between screen time and anxiety in teens.\",\n            \"context\": \"Public health forum\",\n            \"label_id\": 0  # support\n        }\n    ]\n    \n    # Function to train the model\n    def train_model_from_ui(train_data_json, epochs, batch_size):\n        try:\n            # Parse training data\n            train_data = json.loads(train_data_json)\n            \n            # Validate data format\n            if not isinstance(train_data, list):\n                return \"Error: Training data must be a list of examples\"\n            \n            for item in train_data:\n                if not all(k in item for k in [\"source_text\", \"target_text\", \"label_id\"]):\n                    return \"Error: Each training example must have source_text, target_text, and label_id\"\n            \n            # Train the model\n            result = rbam_model.train_model(\n                train_data=train_data,\n                epochs=int(epochs),\n                batch_size=int(batch_size),\n                output_dir=\"models/rbam-custom\"\n            )\n            \n            return f\"Training completed: {result['message']}\"\n        except Exception as e:\n            return f\"Training error: {str(e)}\"\n    \n    # Function to predict relations\n    def predict_relation_from_ui(source_text, target_text, context):\n        if not source_text or not target_text:\n            return \"Error: Source and target arguments are required\"\n        \n        result = rbam_model.predict_relation(\n            source_text=source_text,\n            target_text=target_text,\n            context=context if context else None\n        )\n        \n        # Format the result for display\n        formatted_result = f\"\"\"\n## Prediction Result\n\n**Relation Type:** {result['relation_type']}\n**Confidence:** {result['confidence']:.4f} ({result['confidence']*100:.1f}%)\n\n### Probability Distribution:\n\"\"\"\n        \n        # Add probability distribution\n        probs = result['features']['probabilities']\n        for rel, prob in probs.items():\n            formatted_result += f\"- {rel}: {prob:.4f} ({prob*100:.1f}%)\\n\"\n        \n        # Add semantic similarity if available\n        if 'similarity' in result['features']:\n            formatted_result += f\"\\n**Semantic Similarity:** {result['features']['similarity']:.4f}\\n\"\n            \n        return formatted_result\n    \n    # Function to analyze argument graph\n    def analyze_graph_from_ui(argument_pairs_json):\n        try:\n            # Parse argument pairs\n            argument_pairs = json.loads(argument_pairs_json)\n            \n            # Validate data format\n            if not isinstance(argument_pairs, list):\n                return \"Error: Argument pairs must be a list\"\n            \n            for item in argument_pairs:\n                if not all(k in item for k in [\"source_text\", \"target_text\"]):\n                    return \"Error: Each pair must have source_text and target_text\"\n            \n            # Analyze the graph\n            result = rbam_model.analyze_graph(argument_pairs)\n            \n            # Format the result for display\n            formatted_result = f\"\"\"\n## Graph Analysis Result\n\n**Arguments:** {result['argument_count']}\n**Relations:** {result['relation_count']}\n\n### Key Arguments:\n\"\"\"\n            \n            # Add key arguments\n            for i, arg in enumerate(result['key_arguments']):\n                formatted_result += f\"{i+1}. **{arg['text']}**\\n\"\n                formatted_result += f\"   - Support: {arg['support_centrality']:.2f}\\n\"\n                formatted_result += f\"   - Attack: {arg['attack_centrality']:.2f}\\n\"\n                formatted_result += f\"   - Total: {arg['total_centrality']:.2f}\\n\\n\"\n            \n            # Add inconsistencies\n            formatted_result += f\"\\n### Inconsistencies ({len(result['inconsistencies'])}):\\n\"\n            \n            for i, inconsistency in enumerate(result['inconsistencies']):\n                formatted_result += f\"{i+1}. **{inconsistency['type']}**\\n\"\n                formatted_result += f\"   {inconsistency['description']}\\n\\n\"\n            \n            return formatted_result\n        except Exception as e:\n            return f\"Analysis error: {str(e)}\"\n    \n    # Create the Gradio interface\n    with gr.Blocks(title=\"Enhanced RBAM System\") as demo:\n        gr.Markdown(\"# Enhanced Relation-Based Argument Mining (RBAM) System\")\n        gr.Markdown(f\"**Model Status:** {model_status}\")\n        \n        with gr.Tab(\"Model Training\"):\n            gr.Markdown(\"### Train the RBAM Model\")\n            gr.Markdown(\"Use this tab to fine-tune the model on your own data.\")\n            \n            train_data_input = gr.Textbox(\n                label=\"Training Data (JSON format)\",\n                placeholder=json.dumps(sample_train_data, indent=2),\n                lines=10\n            )\n            \n            with gr.Row():\n                epochs_input = gr.Number(label=\"Epochs\", value=3, minimum=1)\n                batch_size_input = gr.Number(label=\"Batch Size\", value=8, minimum=1)\n            \n            train_button = gr.Button(\"Train Model\")\n            train_output = gr.Textbox(label=\"Training Result\")\n            \n            train_button.click(\n                fn=train_model_from_ui,\n                inputs=[train_data_input, epochs_input, batch_size_input],\n                outputs=train_output\n            )\n        \n        with gr.Tab(\"Relation Prediction\"):\n            gr.Markdown(\"### Predict Relations Between Arguments\")\n            \n            source_input = gr.Textbox(label=\"Source Argument\", lines=4)\n            target_input = gr.Textbox(label=\"Target Argument\", lines=4)\n            context_input = gr.Textbox(label=\"Context (optional)\", lines=2)\n            \n            predict_button = gr.Button(\"Predict Relation\")\n            prediction_output = gr.Markdown(label=\"Prediction Result\")\n            \n            predict_button.click(\n                fn=predict_relation_from_ui,\n                inputs=[source_input, target_input, context_input],\n                outputs=prediction_output\n            )\n        \n        with gr.Tab(\"Graph Analysis\"):\n            gr.Markdown(\"### Analyze Argument Network\")\n            \n            sample_graph_data = [\n                {\"source_text\": \"Climate change is primarily caused by human activities.\", \n                 \"target_text\": \"The rise in global temperatures correlates with increased CO2 emissions.\"},\n                {\"source_text\": \"Renewable energy is too expensive to replace fossil fuels.\", \n                 \"target_text\": \"Solar panel costs have decreased by 90% in the last decade.\"},\n                {\"source_text\": \"Solar panel costs have decreased by 90% in the last decade.\", \n                 \"target_text\": \"Renewable energy is becoming more economically viable.\"}\n            ]\n            \n            graph_data_input = gr.Textbox(\n                label=\"Argument Pairs (JSON format)\",\n                placeholder=json.dumps(sample_graph_data, indent=2),\n                lines=10\n            )\n            \n            analyze_button = gr.Button(\"Analyze Graph\")\n            analysis_output = gr.Markdown(label=\"Analysis Result\")\n            \n            analyze_button.click(\n                fn=analyze_graph_from_ui,\n                inputs=graph_data_input,\n                outputs=analysis_output\n            )\n    \n    # Launch the interface\n    demo.launch(share=True)\n    return \"Interactive RBAM system started\"\n\n# Run the system when this cell is executed\nif __name__ == \"__main__\":\n    # Install required packages if not already installed\n    try:\n        import gradio\n    except ImportError:\n        print(\"Installing required packages...\")\n        import pip\n        pip.main(['install', 'gradio', 'transformers', 'sentence-transformers', 'torch', 'datasets'])\n    \n    print(\"Starting Enhanced RBAM Interactive System...\")\n    run_interactive_rbam_system()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:00:18.548687Z","iopub.execute_input":"2025-08-01T19:00:18.549090Z","iopub.status.idle":"2025-08-01T19:01:22.221503Z","shell.execute_reply.started":"2025-08-01T19:00:18.549056Z","shell.execute_reply":"2025-08-01T19:01:22.220491Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\nRequirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.5.1)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","output_type":"stream"},{"name":"stderr","text":"2025-08-01 19:00:43.465691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754074843.726232      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754074843.795063      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Starting Enhanced RBAM Interactive System...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://5097fa65f43e6b9303.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://5097fa65f43e6b9303.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"Codo with file loading access","metadata":{}},{"cell_type":"code","source":"!pip install gradio\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Optional, Union\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, Trainer, TrainingArguments\nfrom sentence_transformers import SentenceTransformer\nimport logging\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport datasets\nfrom torch.utils.data import Dataset\nimport json\nimport gradio as gr\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass ArgumentRelationDataset(Dataset):\n    \"\"\"Custom dataset for argument relation data\"\"\"\n    \n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n        \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        source = item[\"source_text\"]\n        target = item[\"target_text\"]\n        context = item.get(\"context\", \"\")\n        \n        # Prepare text input\n        if context:\n            text = f\"Context: {context}\\nSource: {source}\\nTarget: {target}\"\n        else:\n            text = f\"Source: {source}\\nTarget: {target}\"\n            \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Remove batch dimension\n        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n        \n        # Add label\n        encoding[\"labels\"] = torch.tensor(item[\"label_id\"])\n        \n        return encoding\n\ndef load_data_from_json_files(file_paths: List[str]) -> List[Dict]:\n    \"\"\"\n    Load and combine data from multiple JSON files.\n    \n    Args:\n        file_paths: List of paths to JSON files\n        \n    Returns:\n        Combined list of data entries from all files\n    \"\"\"\n    combined_data = []\n    \n    for file_path in file_paths:\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                \n                # Handle both list and dictionary formats\n                if isinstance(data, list):\n                    combined_data.extend(data)\n                elif isinstance(data, dict) and 'items' in data:\n                    combined_data.extend(data['items'])\n                elif isinstance(data, dict):\n                    combined_data.append(data)\n                    \n                logger.info(f\"Loaded {len(data) if isinstance(data, list) else 1} items from {file_path}\")\n                \n        except Exception as e:\n            logger.error(f\"Error loading {file_path}: {str(e)}\")\n    \n    return combined_data\n\nclass EnhancedRBAMModel:\n    \"\"\"\n    Enhanced Relation-Based Argument Mining Model\n    \n    This model combines transformer-based classification with semantic similarity\n    analysis and contextual argument clustering to provide more accurate relation\n    detection between arguments.\n    \"\"\"\n    \n    def __init__(self, \n                 model_path: str = \"models/deberta-v3-large-relation-finetuned\",\n                 embedding_model: str = \"all-MiniLM-L6-v2\",\n                 device: str = None):\n        \"\"\"\n        Initialize the enhanced RBAM model.\n        \n        Args:\n            model_path: Path to the fine-tuned classification model\n            embedding_model: SentenceTransformer model for semantic analysis\n            device: Device to run the model on (None for auto-detection)\n        \"\"\"\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Using device: {self.device}\")\n        \n        # Load classification model\n        try:\n            self.model_path = model_path\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n            self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)\n            self.model.eval()\n            logger.info(f\"Successfully loaded classification model from {model_path}\")\n            \n            # Determine relation labels from model config\n            self.id2label = self.model.config.id2label if hasattr(self.model.config, 'id2label') else {\n                0: \"support\", 1: \"attack\", 2: \"neutral\", 3: \"detail\"\n            }\n            self.label2id = {v: k for k, v in self.id2label.items()}\n            logger.info(f\"Loaded relation labels: {self.id2label}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load classification model: {e}\")\n            raise Exception(f\"Model initialization failed: {str(e)}\")\n            \n        # Load embedding model for semantic analysis\n        try:\n            self.embedding_model = SentenceTransformer(embedding_model).to(self.device)\n            logger.info(f\"Successfully loaded embedding model: {embedding_model}\")\n        except Exception as e:\n            logger.error(f\"Failed to load embedding model: {e}\")\n            self.embedding_model = None\n            \n        # Initialize cache\n        self.cache = {}\n        \n    def train_model(self, train_data: List[Dict], validation_data: Optional[List[Dict]] = None, \n                   output_dir: str = \"./model_output\", epochs: int = 3, batch_size: int = 16):\n        \"\"\"\n        Train or fine-tune the model on new data.\n        \n        Args:\n            train_data: List of dictionaries with source_text, target_text, optional context, and label_id\n            validation_data: Optional validation data with the same format as train_data\n            output_dir: Directory to save the trained model\n            epochs: Number of training epochs\n            batch_size: Batch size for training\n        \"\"\"\n        logger.info(f\"Starting model training with {len(train_data)} examples\")\n        \n        # Create datasets\n        train_dataset = ArgumentRelationDataset(train_data, self.tokenizer)\n        eval_dataset = ArgumentRelationDataset(validation_data, self.tokenizer) if validation_data else None\n        \n        # Define training arguments\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=100,\n            evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True if eval_dataset else False,\n        )\n        \n        # Create trainer\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset\n        )\n        \n        # Train model\n        trainer.train()\n        \n        # Save model\n        self.model.save_pretrained(output_dir)\n        self.tokenizer.save_pretrained(output_dir)\n        \n        # Update current model\n        self.model = trainer.model\n        logger.info(f\"Training completed, model saved to {output_dir}\")\n        \n        # Clear cache after training\n        self.cache = {}\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Model trained on {len(train_data)} examples and saved to {output_dir}\"\n        }\n        \n    def predict_relation(self, \n                        source_text: str, \n                        target_text: str, \n                        context: Optional[str] = None) -> Dict:\n        \"\"\"\n        Predict the relation between source and target arguments.\n        \n        Args:\n            source_text: The source argument text\n            target_text: The target argument text\n            context: Optional context for the arguments\n            \n        Returns:\n            Dictionary with relation type, confidence score, and features\n        \"\"\"\n        # Create cache key\n        cache_key = f\"{source_text[:100]}|{target_text[:100]}|{context[:100] if context else 'no_context'}\"\n        \n        # Check if result is cached\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n            \n        # Prepare input\n        input_text = self._prepare_input(source_text, target_text, context)\n        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n        \n        # Get model prediction\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits\n            probabilities = torch.nn.functional.softmax(logits, dim=1)[0]\n            prediction = torch.argmax(probabilities).item()\n            confidence = probabilities[prediction].item()\n            \n        # Get additional semantic features if embedding model is available\n        semantic_features = {}\n        if self.embedding_model:\n            semantic_features = self._get_semantic_features(source_text, target_text)\n        \n        # Ensemble the predictions\n        relation_type = self.id2label[prediction]\n        \n        # Apply confidence adjustment based on semantic features\n        if semantic_features and 'similarity' in semantic_features:\n            # Adjust confidence based on semantic similarity\n            if relation_type in ['support', 'detail'] and semantic_features['similarity'] < 0.3:\n                confidence = confidence * 0.8  # Reduce confidence for support/detail with low similarity\n            elif relation_type == 'attack' and semantic_features['similarity'] > 0.8:\n                confidence = confidence * 0.8  # Reduce confidence for attack with high similarity\n        \n        # Build result\n        result = {\n            'relation_type': relation_type,\n            'confidence': confidence,\n            'features': {\n                'probabilities': {self.id2label[i]: prob.item() for i, prob in enumerate(probabilities)},\n                **semantic_features\n            }\n        }\n        \n        # Cache the result\n        self.cache[cache_key] = result\n        \n        return result\n    \n    def _prepare_input(self, source_text: str, target_text: str, context: Optional[str] = None) -> str:\n        \"\"\"\n        Prepare input text for the model.\n        \"\"\"\n        if context:\n            return f\"Context: {context}\\nSource: {source_text}\\nTarget: {target_text}\"\n        else:\n            return f\"Source: {source_text}\\nTarget: {target_text}\"\n    \n    def _get_semantic_features(self, source_text: str, target_text: str) -> Dict:\n        \"\"\"\n        Extract semantic features for the argument pair.\n        \"\"\"\n        # Get embeddings\n        source_embedding = self.embedding_model.encode(source_text, convert_to_tensor=True)\n        target_embedding = self.embedding_model.encode(target_text, convert_to_tensor=True)\n        \n        # Calculate similarity\n        similarity = torch.cosine_similarity(source_embedding.unsqueeze(0), \n                                           target_embedding.unsqueeze(0)).item()\n        \n        return {\n            'similarity': similarity,\n        }\n    \n    def batch_predict(self, argument_pairs: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Predict relations for multiple argument pairs.\n        \n        Args:\n            argument_pairs: List of dictionaries with source_text, target_text, and optional context\n            \n        Returns:\n            List of prediction dictionaries\n        \"\"\"\n        results = []\n        \n        for pair in argument_pairs:\n            source = pair.get('source_text')\n            target = pair.get('target_text')\n            context = pair.get('context')\n            \n            if not source or not target:\n                results.append({'error': 'Missing source or target text'})\n                continue\n                \n            result = self.predict_relation(source, target, context)\n            results.append(result)\n            \n        return results\n    \n    def analyze_graph(self, argument_pairs: List[Dict]) -> Dict:\n        \"\"\"\n        Perform graph-based analysis of the argument network.\n        \n        Args:\n            argument_pairs: List of dictionaries with source_text, target_text, and optional context\n            \n        Returns:\n            Dictionary with graph analysis results\n        \"\"\"\n        # Get predictions for all pairs\n        predictions = self.batch_predict(argument_pairs)\n        \n        # Extract unique arguments\n        unique_args = set()\n        for pair in argument_pairs:\n            unique_args.add(pair['source_text'])\n            unique_args.add(pair['target_text'])\n        \n        # Build adjacency matrix\n        n_args = len(unique_args)\n        arg_list = list(unique_args)\n        arg_to_idx = {arg: i for i, arg in enumerate(arg_list)}\n        \n        # Initialize matrices\n        support_matrix = np.zeros((n_args, n_args))\n        attack_matrix = np.zeros((n_args, n_args))\n        \n        # Fill matrices\n        for i, pair in enumerate(argument_pairs):\n            source_idx = arg_to_idx[pair['source_text']]\n            target_idx = arg_to_idx[pair['target_text']]\n            rel_type = predictions[i]['relation_type']\n            confidence = predictions[i]['confidence']\n            \n            if rel_type == 'support':\n                support_matrix[source_idx, target_idx] = confidence\n            elif rel_type == 'attack':\n                attack_matrix[source_idx, target_idx] = confidence\n        \n        # Calculate centrality metrics\n        support_centrality = np.sum(support_matrix, axis=0)\n        attack_centrality = np.sum(attack_matrix, axis=0)\n        \n        # Identify key arguments\n        key_args = []\n        for i, arg in enumerate(arg_list):\n            total_centrality = support_centrality[i] + attack_centrality[i]\n            if total_centrality > 0:\n                key_args.append({\n                    'text': arg[:100] + '...' if len(arg) > 100 else arg,\n                    'support_centrality': float(support_centrality[i]),\n                    'attack_centrality': float(attack_centrality[i]),\n                    'total_centrality': float(total_centrality)\n                })\n        \n        # Sort by total centrality\n        key_args.sort(key=lambda x: x['total_centrality'], reverse=True)\n        \n        # Identify inconsistencies\n        inconsistencies = self._identify_inconsistencies(argument_pairs, predictions)\n        \n        return {\n            'key_arguments': key_args[:5],  # Top 5 key arguments\n            'argument_count': len(unique_args),\n            'relation_count': len(argument_pairs),\n            'inconsistencies': inconsistencies\n        }\n    \n    def _identify_inconsistencies(self, argument_pairs: List[Dict], predictions: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Identify potential inconsistencies in the argument network.\n        \"\"\"\n        inconsistencies = []\n        \n        # Build graph\n        graph = {}\n        for i, pair in enumerate(argument_pairs):\n            source = pair['source_text']\n            target = pair['target_text']\n            rel_type = predictions[i]['relation_type']\n            \n            if source not in graph:\n                graph[source] = {}\n            if target not in graph:\n                graph[target] = {}\n                \n            graph[source][target] = rel_type\n        \n        # Check for circular support/attack patterns\n        for source in graph:\n            for target in graph.get(source, {}):\n                rel_type = graph[source][target]\n                \n                # Check if there's a reverse relation\n                if target in graph and source in graph.get(target, {}):\n                    reverse_rel = graph[target][source]\n                    \n                    # Check if there's an inconsistency\n                    if rel_type == reverse_rel and rel_type in ['support', 'attack']:\n                        inconsistencies.append({\n                            'type': 'circular_relation',\n                            'source': source[:50] + '...' if len(source) > 50 else source,\n                            'target': target[:50] + '...' if len(target) > 50 else target,\n                            'relation': rel_type,\n                            'description': f\"Circular {rel_type} relation detected\"\n                        })\n        \n        # Check for support-attack contradictions\n        for arg1 in graph:\n            for arg2 in graph.get(arg1, {}):\n                rel1 = graph[arg1][arg2]\n                \n                # Check if arg1 and arg2 have relations to the same arg3\n                for arg3 in graph:\n                    if arg3 != arg1 and arg3 != arg2 and arg2 in graph.get(arg3, {}):\n                        rel2 = graph[arg3][arg2]\n                        \n                        # Check if one supports while the other attacks\n                        if rel1 == 'support' and rel2 == 'attack' or rel1 == 'attack' and rel2 == 'support':\n                            inconsistencies.append({\n                                'type': 'competing_relations',\n                                'arguments': [\n                                    arg1[:50] + '...' if len(arg1) > 50 else arg1,\n                                    arg2[:50] + '...' if len(arg2) > 50 else arg2,\n                                    arg3[:50] + '...' if len(arg3) > 50 else arg3\n                                ],\n                                'description': f\"Competing {rel1}/{rel2} relations detected\"\n                            })\n        \n        return inconsistencies\n\ndef save_uploaded_files(files):\n    \"\"\"\n    Save uploaded files to a temporary directory and return their paths.\n    \"\"\"\n    if not os.path.exists(\"temp_uploads\"):\n        os.makedirs(\"temp_uploads\")\n        \n    file_paths = []\n    for i, file in enumerate(files):\n        temp_path = os.path.join(\"temp_uploads\", f\"upload_{i}_{os.path.basename(file.name)}\")\n        \n        # Fix: Handle both file-like objects and Gradio's UploadedFile objects\n        if hasattr(file, 'read'):\n            # File is a file-like object\n            with open(temp_path, \"wb\") as f:\n                f.write(file.read())\n        else:\n            # File might be a path string from Gradio\n            if isinstance(file, str):\n                # It's already a file path\n                temp_path = file\n            else:\n                # Copy the file content\n                with open(file.name, \"rb\") as src, open(temp_path, \"wb\") as dst:\n                    dst.write(src.read())\n                    \n        file_paths.append(temp_path)\n    \n    return file_paths\n\n# Create a function to run the interactive web UI\ndef run_interactive_rbam_system():\n    # Create the model\n    model_path = \"distilbert-base-uncased\"  # Default model, will be fine-tuned\n    \n    try:\n        rbam_model = EnhancedRBAMModel(model_path=model_path)\n        model_status = \"Model loaded successfully\"\n    except Exception as e:\n        # If loading fails, use a fallback approach\n        from transformers import DistilBertForSequenceClassification\n        model = DistilBertForSequenceClassification.from_pretrained(\n            \"distilbert-base-uncased\", \n            num_labels=4\n        )\n        model.config.id2label = {0: \"support\", 1: \"attack\", 2: \"neutral\", 3: \"detail\"}\n        model.config.label2id = {\"support\": 0, \"attack\": 1, \"neutral\": 2, \"detail\": 3}\n        \n        # Create output directory if it doesn't exist\n        os.makedirs(\"models/distilbert-rbam\", exist_ok=True)\n        model.save_pretrained(\"models/distilbert-rbam\")\n        \n        # Now initialize with the saved model\n        rbam_model = EnhancedRBAMModel(model_path=\"models/distilbert-rbam\")\n        model_status = \"Initialized with base model (requires training)\"\n    \n    # Sample training data\n    sample_train_data = [\n        {\n            \"source_text\": \"Climate change is primarily caused by human activities.\",\n            \"target_text\": \"The rise in global temperatures correlates with increased CO2 emissions.\",\n            \"context\": \"Environmental science debate\",\n            \"label_id\": 0  # support\n        },\n        {\n            \"source_text\": \"Renewable energy is too expensive to replace fossil fuels.\",\n            \"target_text\": \"Solar panel costs have decreased by 90% in the last decade.\",\n            \"context\": \"Energy policy discussion\",\n            \"label_id\": 1  # attack\n        },\n        {\n            \"source_text\": \"Excessive social media use is harmful to mental health.\",\n            \"target_text\": \"Studies show correlation between screen time and anxiety in teens.\",\n            \"context\": \"Public health forum\",\n            \"label_id\": 0  # support\n        }\n    ]\n    \n    # Sample argument network data\n    sample_graph_data = [\n        {\"source_text\": \"Climate change is primarily caused by human activities.\", \n         \"target_text\": \"The rise in global temperatures correlates with increased CO2 emissions.\"},\n        {\"source_text\": \"Renewable energy is too expensive to replace fossil fuels.\", \n         \"target_text\": \"Solar panel costs have decreased by 90% in the last decade.\"},\n        {\"source_text\": \"Solar panel costs have decreased by 90% in the last decade.\", \n         \"target_text\": \"Renewable energy is becoming more economically viable.\"}\n    ]\n    \n    # Function to train the model\n    def train_model_from_ui(file_inputs, train_data_json, epochs, batch_size):\n        try:\n            train_data = []\n            \n            # Process uploaded files if any\n            if file_inputs:\n                file_paths = save_uploaded_files(file_inputs)\n                loaded_data = load_data_from_json_files(file_paths)\n                train_data.extend(loaded_data)\n                \n            # Process pasted JSON if not empty\n            if train_data_json.strip():\n                pasted_data = json.loads(train_data_json)\n                if isinstance(pasted_data, list):\n                    train_data.extend(pasted_data)\n                else:\n                    train_data.append(pasted_data)\n            \n            # Validate data format\n            if not train_data:\n                return \"Error: No training data provided\"\n                \n            for item in train_data:\n                if not all(k in item for k in [\"source_text\", \"target_text\", \"label_id\"]):\n                    return \"Error: Each training example must have source_text, target_text, and label_id\"\n            \n            # Train the model\n            result = rbam_model.train_model(\n                train_data=train_data,\n                epochs=int(epochs),\n                batch_size=int(batch_size),\n                output_dir=\"models/rbam-custom\"\n            )\n            \n            return f\"Training completed with {len(train_data)} examples: {result['message']}\"\n        except Exception as e:\n            return f\"Training error: {str(e)}\"\n    \n    # Function to predict relations\n    def predict_relation_from_ui(source_text, target_text, context):\n        if not source_text or not target_text:\n            return \"Error: Source and target arguments are required\"\n        \n        result = rbam_model.predict_relation(\n            source_text=source_text,\n            target_text=target_text,\n            context=context if context else None\n        )\n        \n        # Format the result for display\n        formatted_result = f\"\"\"\n## Prediction Result\n\n**Relation Type:** {result['relation_type']}\n**Confidence:** {result['confidence']:.4f} ({result['confidence']*100:.1f}%)\n\n### Probability Distribution:\n\"\"\"\n        \n        # Add probability distribution\n        probs = result['features']['probabilities']\n        for rel, prob in probs.items():\n            formatted_result += f\"- {rel}: {prob:.4f} ({prob*100:.1f}%)\\n\"\n        \n        # Add semantic similarity if available\n        if 'similarity' in result['features']:\n            formatted_result += f\"\\n**Semantic Similarity:** {result['features']['similarity']:.4f}\\n\"\n            \n        return formatted_result\n    \n    # Function to analyze argument graph\n    def analyze_graph_from_ui(file_inputs, graph_data_json):\n        try:\n            argument_pairs = []\n            \n            # Process uploaded files if any\n            if file_inputs:\n                file_paths = save_uploaded_files(file_inputs)\n                loaded_data = load_data_from_json_files(file_paths)\n                argument_pairs.extend(loaded_data)\n                \n            # Process pasted JSON if not empty\n            if graph_data_json.strip():\n                pasted_data = json.loads(graph_data_json)\n                if isinstance(pasted_data, list):\n                    argument_pairs.extend(pasted_data)\n                else:\n                    argument_pairs.append(pasted_data)\n            \n            # Validate data format\n            if not argument_pairs:\n                return \"Error: No argument pairs provided\"\n                \n            for item in argument_pairs:\n                if not all(k in item for k in [\"source_text\", \"target_text\"]):\n                    return \"Error: Each pair must have source_text and target_text\"\n            \n            # Analyze the graph\n            result = rbam_model.analyze_graph(argument_pairs)\n            \n            # Format the result for display\n            formatted_result = f\"\"\"\n## Graph Analysis Result\n\n**Arguments:** {result['argument_count']}\n**Relations:** {result['relation_count']}\n\n### Key Arguments:\n\"\"\"\n            \n            # Add key arguments\n            for i, arg in enumerate(result['key_arguments']):\n                formatted_result += f\"{i+1}. **{arg['text']}**\\n\"\n                formatted_result += f\"   - Support: {arg['support_centrality']:.2f}\\n\"\n                formatted_result += f\"   - Attack: {arg['attack_centrality']:.2f}\\n\"\n                formatted_result += f\"   - Total: {arg['total_centrality']:.2f}\\n\\n\"\n            \n            # Add inconsistencies\n            formatted_result += f\"\\n### Inconsistencies ({len(result['inconsistencies'])}):\\n\"\n            \n            for i, inconsistency in enumerate(result['inconsistencies']):\n                formatted_result += f\"{i+1}. **{inconsistency['type']}**\\n\"\n                formatted_result += f\"   {inconsistency['description']}\\n\\n\"\n            \n            return formatted_result\n        except Exception as e:\n            return f\"Analysis error: {str(e)}\"\n    \n    # Create the Gradio interface\n    with gr.Blocks(title=\"Enhanced RBAM System\") as demo:\n        gr.Markdown(\"# Enhanced Relation-Based Argument Mining (RBAM) System\")\n        gr.Markdown(f\"**Model Status:** {model_status}\")\n        \n        with gr.Tab(\"Model Training\"):\n            gr.Markdown(\"### Train the RBAM Model\")\n            gr.Markdown(\"Use this tab to fine-tune the model on your own data.\")\n            \n            with gr.Row():\n                with gr.Column(scale=1):\n                    gr.Markdown(\"**Option 1: Upload JSON Files**\")\n                    file_inputs = gr.File(\n                        file_count=\"multiple\",\n                        label=\"Upload JSON Files\",\n                        file_types=[\".json\"]\n                    )\n                \n                with gr.Column(scale=1):\n                    gr.Markdown(\"**Option 2: Paste JSON Data**\")\n                    train_data_input = gr.Textbox(\n                        label=\"Training Data (JSON format)\",\n                        placeholder=json.dumps(sample_train_data, indent=2),\n                        lines=10\n                    )\n            \n            with gr.Row():\n                epochs_input = gr.Number(label=\"Epochs\", value=3, minimum=1)\n                batch_size_input = gr.Number(label=\"Batch Size\", value=8, minimum=1)\n            \n            train_button = gr.Button(\"Train Model\")\n            train_output = gr.Textbox(label=\"Training Result\")\n            \n            train_button.click(\n                fn=train_model_from_ui,\n                inputs=[file_inputs, train_data_input, epochs_input, batch_size_input],\n                outputs=train_output\n            )\n        \n        with gr.Tab(\"Relation Prediction\"):\n            gr.Markdown(\"### Predict Relations Between Arguments\")\n            \n            source_input = gr.Textbox(label=\"Source Argument\", lines=4)\n            target_input = gr.Textbox(label=\"Target Argument\", lines=4)\n            context_input = gr.Textbox(label=\"Context (optional)\", lines=2)\n            \n            predict_button = gr.Button(\"Predict Relation\")\n            prediction_output = gr.Markdown(label=\"Prediction Result\")\n            \n            predict_button.click(\n                fn=predict_relation_from_ui,\n                inputs=[source_input, target_input, context_input],\n                outputs=prediction_output\n            )\n        \n        with gr.Tab(\"Graph Analysis\"):\n            gr.Markdown(\"### Analyze Argument Network\")\n            \n            with gr.Row():\n                with gr.Column(scale=1):\n                    gr.Markdown(\"**Option 1: Upload JSON Files**\")\n                    graph_file_inputs = gr.File(\n                        file_count=\"multiple\",\n                        label=\"Upload JSON Files with Argument Pairs\",\n                        file_types=[\".json\"]\n                    )\n                \n                with gr.Column(scale=1):\n                    gr.Markdown(\"**Option 2: Paste JSON Data**\")\n                    graph_data_input = gr.Textbox(\n                        label=\"Argument Pairs (JSON format)\",\n                        placeholder=json.dumps(sample_graph_data, indent=2),\n                        lines=10\n                    )\n            \n            analyze_button = gr.Button(\"Analyze Graph\")\n            analysis_output = gr.Markdown(label=\"Analysis Result\")\n            \n            analyze_button.click(\n                fn=analyze_graph_from_ui,\n                inputs=[graph_file_inputs, graph_data_input],\n                outputs=analysis_output\n            )\n        \n        with gr.Tab(\"Help\"):\n            gr.Markdown(\"\"\"\n            ## How to Use the RBAM System\n            \n            ### Data Format\n            \n            #### Training Data Format\n            ```json\n            [\n                {\n                    \"source_text\": \"First argument text\",\n                    \"target_text\": \"Second argument text\",\n                    \"context\": \"Optional context information\",\n                    \"label_id\": 0  // 0: support, 1: attack, 2: neutral, 3: detail\n                },\n                ...\n            ]\n            ```\n            \n            #### Argument Pairs Format for Graph Analysis\n            ```json\n            [\n                {\n                    \"source_text\": \"First argument text\",\n                    \"target_text\": \"Second argument text\",\n                    \"context\": \"Optional context information\"\n                },\n                ...\n            ]\n            ```\n            \n            ### Relation Types\n            - **Support**: Source argument supports or reinforces the target argument\n            - **Attack**: Source argument contradicts or weakens the target argument\n            - **Neutral**: Source argument neither supports nor attacks the target argument\n            - **Detail**: Source argument provides additional details about the target argument\n            \n            ### Tips\n            - For best results, provide clear and concise arguments\n            - Context is optional but can improve prediction accuracy\n            - The model performs better when trained on domain-specific data\n            - In graph analysis, larger networks may take longer to process\n            \"\"\")\n    \n    # Launch the interface\n    demo.launch(share=True)\n    return \"Interactive RBAM system started\"\n\n# Run the system when this script is executed\nif __name__ == \"__main__\":\n    # Install required packages if not already installed\n    try:\n        import gradio\n    except ImportError:\n        print(\"Installing required packages...\")\n        import pip\n        pip.main(['install', 'gradio', 'transformers', 'sentence-transformers', 'torch', 'datasets'])\n    \n    print(\"Starting Enhanced RBAM Interactive System...\")\n    run_interactive_rbam_system()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:02:06.051169Z","iopub.execute_input":"2025-08-01T19:02:06.052199Z","iopub.status.idle":"2025-08-01T19:02:12.893875Z","shell.execute_reply.started":"2025-08-01T19:02:06.052166Z","shell.execute_reply":"2025-08-01T19:02:12.892862Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\nRequirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.5.1)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nStarting Enhanced RBAM Interactive System...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7863\n* Running on public URL: https://9e2ee11232c31f4df3.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://9e2ee11232c31f4df3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":4}]}